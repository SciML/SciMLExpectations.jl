<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators · SciMLExpectations.jl</title><meta name="title" content="GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators · SciMLExpectations.jl"/><meta property="og:title" content="GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators · SciMLExpectations.jl"/><meta property="twitter:title" content="GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators · SciMLExpectations.jl"/><meta name="description" content="Documentation for SciMLExpectations.jl."/><meta property="og:description" content="Documentation for SciMLExpectations.jl."/><meta property="twitter:description" content="Documentation for SciMLExpectations.jl."/><meta property="og:url" content="https://docs.sciml.ai/SciMLExpectations/stable/tutorials/gpu_bayesian/"/><meta property="twitter:url" content="https://docs.sciml.ai/SciMLExpectations/stable/tutorials/gpu_bayesian/"/><link rel="canonical" href="https://docs.sciml.ai/SciMLExpectations/stable/tutorials/gpu_bayesian/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SciMLExpectations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SciMLExpectations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../introduction/">An Introduction to Expectations via SciMLExpectations.jl</a></li><li><a class="tocitem" href="../optimization_under_uncertainty/">Optimization Under Uncertainty</a></li><li class="is-active"><a class="tocitem" href>GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators</a><ul class="internal"><li><a class="tocitem" href="#Bayesian-Parameter-Estimation-with-Uncertainty"><span>Bayesian Parameter Estimation with Uncertainty</span></a></li><li><a class="tocitem" href="#Evaluating-Model-Hypotheses-with-the-Koopman-Expectation"><span>Evaluating Model Hypotheses with the Koopman Expectation</span></a></li></ul></li><li><a class="tocitem" href="../process_noise/">Expectation of Process Noise</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../../manual/problem/">ExpectationProblem</a></li><li><a class="tocitem" href="../../manual/solve/">Solving Expectation Problems</a></li><li><a class="tocitem" href="../../manual/algorithms/">Expectation Algorithms</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/SciMLExpectations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/SciMLExpectations.jl/blob/master/docs/src/tutorials/gpu_bayesian.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="GPU-Accelerated-Data-Driven-Bayesian-Uncertainty-Quantification-with-Koopman-Operators"><a class="docs-heading-anchor" href="#GPU-Accelerated-Data-Driven-Bayesian-Uncertainty-Quantification-with-Koopman-Operators">GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators</a><a id="GPU-Accelerated-Data-Driven-Bayesian-Uncertainty-Quantification-with-Koopman-Operators-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Accelerated-Data-Driven-Bayesian-Uncertainty-Quantification-with-Koopman-Operators" title="Permalink"></a></h1><p>What if you have data and a general model and would like to evaluate the probability that the fitted model outcomes would have had a given behavior? The purpose of this tutorial is to demonstrate a fast workflow for doing exactly this. It composes together a few different pieces of the SciML ecosystem:</p><ol><li>Parameter estimation with uncertainty with Bayesian differential equations by integrating the differentiable differential equation solvers with the <a href="https://turinglang.org/stable/">Turing.jl library</a>.</li><li>Fast calculation of probabilistic estimates of differential equation solutions with parametric uncertainty using the Koopman expectation.</li><li>GPU-acceleration of batched differential equation solves.</li></ol><p>Let&#39;s dive right in.</p><h2 id="Bayesian-Parameter-Estimation-with-Uncertainty"><a class="docs-heading-anchor" href="#Bayesian-Parameter-Estimation-with-Uncertainty">Bayesian Parameter Estimation with Uncertainty</a><a id="Bayesian-Parameter-Estimation-with-Uncertainty-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Parameter-Estimation-with-Uncertainty" title="Permalink"></a></h2><p>Let&#39;s start by importing all the necessary libraries:</p><pre><code class="language-julia hljs">using OrdinaryDiffEq
using Turing, MCMCChains, Distributions
using KernelDensity
using SciMLExpectations, IntegralsCuba
using DiffEqGPU
using Plots, StatsPlots
using Random;
Random.seed!(1);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><p>For this tutorial, we will use the Lotka-Volterra equation:</p><pre><code class="language-julia hljs">function lotka_volterra(du, u, p, t)
    @inbounds begin
        x = u[1]
        y = u[2]
        α = p[1]
        β = p[2]
        γ = p[3]
        δ = p[4]
        du[1] = (α - β * y) * x
        du[2] = (δ * x - γ) * y
    end
end
p = [1.5, 1.0, 3.0, 1.0]
u0 = [1.0, 1.0]
prob1 = ODEProblem(lotka_volterra, u0, (0.0, 10.0), p)
sol = solve(prob1, Tsit5())
plot(sol)</code></pre><img src="533f1cdc.svg" alt="Example block output"/><p>From the Lotka-Volterra equation, we will generate a dataset with known parameters:</p><pre><code class="language-julia hljs">sol1 = solve(prob1, Tsit5(), saveat = 0.1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
Interpolation: 1st order linear
t: 101-element Vector{Float64}:
  0.0
  0.1
  0.2
  0.3
  0.4
  0.5
  0.6
  0.7
  0.8
  0.9
  ⋮
  9.2
  9.3
  9.4
  9.5
  9.6
  9.7
  9.8
  9.9
 10.0
u: 101-element Vector{Vector{Float64}}:
 [1.0, 1.0]
 [1.0610780673356455, 0.8210842775886171]
 [1.1440276717257598, 0.6790526689784505]
 [1.2491712125724483, 0.5668931465841184]
 [1.3776445705636382, 0.4788129513795158]
 [1.531230817748013, 0.41015646708661463]
 [1.7122697558187638, 0.3572654487994841]
 [1.9235782758301574, 0.31734720616177187]
 [2.168391089699407, 0.28838884378732427]
 [2.450250667140253, 0.2690537093960073]
 ⋮
 [1.8172823219555445, 4.064946595043299]
 [1.4427612988383434, 3.5397375780465032]
 [1.208908107884442, 2.9914550030313953]
 [1.068592596962787, 2.482072920162587]
 [0.9910229623276097, 2.0372445701968505]
 [0.957421348475831, 1.6632055724973926]
 [0.9569793912886637, 1.3555870283301144]
 [0.98356090632007, 1.1062868199419809]
 [1.0337581256020674, 0.9063703842886032]</code></pre><p>Now let&#39;s assume our dataset should have noise. We can add this noise in and plot the noisy data against the generating set:</p><pre><code class="language-julia hljs">odedata = Array(sol1) + 0.8 * randn(size(Array(sol1)))
plot(sol1, alpha = 0.3, legend = false);
scatter!(sol1.t, odedata&#39;);</code></pre><img src="96e4b1ec.svg" alt="Example block output"/><p>Now let&#39;s assume that all we know is the data <code>odedata</code> and the model form. What we want to do is use the data to inform us of the parameters, but also get a probabilistic sense of the uncertainty around our parameter estimate. This is done via Bayesian estimation. For a full look at Bayesian estimation of differential equations, look at the <a href="https://turinglang.org/stable/tutorials/10-bayesian-differential-equations/">Bayesian differential equation</a> tutorial from Turing.jl.</p><p>Following that tutorial, we choose a set of priors and perform <code>NUTS</code> sampling to arrive at the MCMC chain:</p><pre><code class="language-julia hljs">Turing.setadbackend(:forwarddiff)

@model function fitlv(data, prob1)
    σ ~ InverseGamma(2, 3) # ~ is the tilde character
    α ~ truncated(Normal(1.5, 0.5), 1.0, 2.0)
    β ~ truncated(Normal(1.2, 0.5), 0.5, 1.5)
    γ ~ truncated(Normal(3.0, 0.5), 2, 4)
    δ ~ truncated(Normal(1.0, 0.5), 0.5, 1.5)

    p = [α, β, γ, δ]
    prob = remake(prob1, p = p)
    predicted = solve(prob, Tsit5(), saveat = 0.1)

    for i in 1:length(predicted)
        data[:, i] ~ MvNormal(predicted[i], σ)
    end
end

model = fitlv(odedata, prob1)

# This next command runs 3 independent chains without using multithreading.
chain = mapreduce(c -&gt; sample(model, NUTS(0.45), 1000), chainscat, 1:3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chains MCMC chain (1000×17×3 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 3
Samples per chain = 1000
Wall duration     = 55.45 seconds
Compute duration  = 49.99 seconds
parameters        = σ, α, β, γ, δ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
 <span class="sgr1"> parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯
 <span class="sgr90">     Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯

           σ    0.8417    0.0431    0.0078    27.7432   109.8148    1.1156     ⋯
           α    1.5174    0.0491    0.0107    21.8564    73.3739    1.1784     ⋯
           β    1.0732    0.0495    0.0094    28.9358   127.7724    1.1281     ⋯
           γ    3.0287    0.1443    0.0298    23.5354    93.4607    1.1650     ⋯
           δ    0.9742    0.0505    0.0104    23.6050    76.1056    1.1623     ⋯
</span><span class="sgr36">                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
 </span><span class="sgr90">     Symbol   Float64   Float64   Float64   Float64   Float64

           σ    0.7666    0.8073    0.8397    0.8728    0.9333
           α    1.4264    1.4813    1.5155    1.5504    1.6238
           β    0.9894    1.0369    1.0640    1.1084    1.1829
           γ    2.7360    2.9281    3.0304    3.1281    3.3134
           δ    0.8724    0.9384    0.9747    1.0111    1.0733
</span></span></code></pre><p>This chain gives a discrete approximation to the probability distribution of our desired quantities. We can plot the chains to see these distributions in action:</p><pre><code class="language-julia hljs">plot(chain)</code></pre><img src="8bfa425f.svg" alt="Example block output"/><p>Great! From our data, we have arrived at a probability distribution for our parameter values.</p><h2 id="Evaluating-Model-Hypotheses-with-the-Koopman-Expectation"><a class="docs-heading-anchor" href="#Evaluating-Model-Hypotheses-with-the-Koopman-Expectation">Evaluating Model Hypotheses with the Koopman Expectation</a><a id="Evaluating-Model-Hypotheses-with-the-Koopman-Expectation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-Model-Hypotheses-with-the-Koopman-Expectation" title="Permalink"></a></h2><p>Now, let&#39;s try to ask a question: what is the expected value of <code>x</code> (the first term in the differential equation) at time <code>t=10</code> given the known uncertainties in our parameters? This is a good tutorial question because all other probabilistic statements can be phrased similarly. Asking a question like, “what is the probability that <code>x(T) &gt; 1</code> at the final time <code>T</code>?”, can similarly be phrased as an expected value (probability statements are expected values of characteristic functions which are 1 if true 0 if false). So in general, the kinds of questions we want to ask and answer are expectations about the solutions of the differential equation.</p><p>The trivial to solve this problem is to sample 100,000 sets of parameters from our parameter distribution given by the Bayesian estimation, solve the ODE 100,000 times, and then take the average. But is 100,000 ODE solves enough? Well it&#39;s hard to tell, and even then, the convergence of this approach is slow. This is the Monte Carlo approach, and it converges to the correct answer by <code>sqrt(N)</code>. Slow.</p><p>However, the <a href="https://arxiv.org/abs/2008.08737">Koopman expectation</a> can converge with much fewer points, allowing the use of higher order quadrature methods to converge exponentially faster in many cases. To use the Koopman expectation, we first need to define our observable function <code>g</code>. This function designates the thing about the solution we wish to calculate the expectation of. Thus for our question “what is the expected value of <code>x</code>at time <code>t=10</code>?” we would simply use:</p><pre><code class="language-julia hljs">function g(sol, p)
    sol[1, end]
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">g (generic function with 1 method)</code></pre><p>Now we need to use the <code>expectation</code> call, where we need to provide our initial condition and parameters as probability distributions. For this case, we will use the same constant <code>u0</code> as before. But, let&#39;s turn our Bayesian MCMC chains into distributions through <a href="https://github.com/JuliaStats/KernelDensity.jl">kernel density estimation</a> (the plots of the distribution above are just KDE plots!).</p><pre><code class="language-julia hljs">p_kde = [kde(vec(Array(chain[:α]))), kde(vec(Array(chain[:β]))),
    kde(vec(Array(chain[:γ]))), kde(vec(Array(chain[:δ])))]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{KernelDensity.UnivariateKDE{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}}:
 KernelDensity.UnivariateKDE{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}(1.3324718591150952:0.00018870493018994144:1.7187508512139054, [9.907373544226772e-6, 1.000033177378512e-5, 1.0161812437736728e-5, 1.0392729763442077e-5, 1.069434817413395e-5, 1.1068286942084171e-5, 1.1516525906696273e-5, 1.2041412213115166e-5, 1.2645668139743549e-5, 1.3332399973364772e-5  …  1.2791587909927182e-5, 1.217384336985039e-5, 1.1634445038793961e-5, 1.117083746526859e-5, 1.0780824484579732e-5, 1.0462563863805618e-5, 1.0214562933680282e-5, 1.0035675159170143e-5, 9.92509772643202e-6, 9.88237009072468e-6])
 KernelDensity.UnivariateKDE{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}(0.883394267330297:0.00019301142994834292:1.278488664434555, [1.2031528841216499e-5, 1.2287766372409692e-5, 1.2631969006804411e-5, 1.306611224372034e-5, 1.3592645361715228e-5, 1.4214499684861437e-5, 1.493509831052009e-5, 1.5758367306162313e-5, 1.6688748381010754e-5, 1.7731213028504555e-5  …  1.4202249829481373e-5, 1.3586199029386181e-5, 1.3063657949619056e-5, 1.2631992557299299e-5, 1.2289041628588127e-5, 1.2033110103865097e-5, 1.1862963970848961e-5, 1.177782664235627e-5, 1.1777376830723885e-5, 1.1861747925140387e-5])
 KernelDensity.UnivariateKDE{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}(2.4802871887330267:0.0005682177709970292:3.6434289659639454, [3.831747204863234e-6, 3.885507411593991e-6, 3.967659508941068e-6, 4.0786874420106756e-6, 4.219230549606934e-6, 4.390085879713013e-6, 4.592210987919287e-6, 4.826727233231409e-6, 5.094923564596954e-6, 5.398260803257671e-6  …  4.858149579484161e-6, 4.621099093715664e-6, 4.415986904572622e-6, 4.2417633488375794e-6, 4.097537666246376e-6, 3.98257549735076e-6, 3.896296855904202e-6, 3.838274573668421e-6, 3.808233222206414e-6, 3.8060485192115134e-6])
 KernelDensity.UnivariateKDE{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}(0.7948065916692116:0.00020431348469314916:1.213036294836088, [1.0414353169174717e-5, 1.0602032376905157e-5, 1.0872104300840846e-5, 1.1226502340644373e-5, 1.1667649407520742e-5, 1.2198470054958221e-5, 1.2822404415580024e-5, 1.354342392811958e-5, 1.4366048913139196e-5, 1.5295368051670977e-5  …  1.2977961284604822e-5, 1.2345670017947785e-5, 1.1801885953538482e-5, 1.1343950152606364e-5, 1.0969661706938805e-5, 1.0677274001003045e-5, 1.0465492434108548e-5, 1.0333473612025856e-5, 1.0280826042152746e-5, 1.0307612366630692e-5])</code></pre><p>Now that we have our observable and our uncertainty distributions, let&#39;s calculate the expected value. Using <code>GenericDistribution</code>, SciMLExpectation can calculate expectations of any distribution-like object that supports evaluating the probability density function, for the <code>Koopman</code> solver algorithm, and generating random numbers for the <code>MonteCarlo</code> solver algorithm.</p><pre><code class="language-julia hljs">pdf_func(x) = prod(pdf.(p_kde, x))
rand_func() = nothing # not needed for Koopman
lb = [p_kde_i.x[begin] for p_kde_i in p_kde] #lower bound for integration
ub = [p_kde_i.x[end] for p_kde_i in p_kde] #upper bound for integration
gd = GenericDistribution(pdf_func, rand_func, lb, ub)
h(x, u, p) = u, x
sm = SystemMap(prob1, Tsit5())
exprob = ExpectationProblem(sm, g, h, gd; nout = 1)
sol = solve(exprob, Koopman(); reltol = 1e-2, abstol = 1e-2)
sol.u</code></pre><p>Note how that gives the expectation and a residual for the error bound!</p><pre><code class="language-julia hljs">sol.resid</code></pre><h3 id="GPU-Accelerated-Expectations"><a class="docs-heading-anchor" href="#GPU-Accelerated-Expectations">GPU-Accelerated Expectations</a><a id="GPU-Accelerated-Expectations-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Accelerated-Expectations" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Batch functionality not yet fully implemented in version 2 of SciMLExpectations.</p></div></div><p>Are we done? No, we need to add some GPUs! As mentioned earlier, probability calculations can take quite a bit of ODE solves, so let&#39;s parallelize across the parameters. <a href="https://github.com/SciML/DiffEqGPU.jl">DiffEqGPU.jl</a> allows you to GPU-parallelize across parameters by using the <a href="https://docs.sciml.ai/DiffEqDocs/stable/features/ensemble/">Ensemble interface</a>. Note that you do not have to do any of the heavy lifting: all the conversion to GPU kernels is done automatically by simply specifying <code>EnsembleGPUArray</code> as the ensembling method. For example:</p><pre><code class="language-julia hljs">function lotka_volterra(du, u, p, t)
    @inbounds begin
        x = u[1]
        y = u[2]
        α = p[1]
        β = p[2]
        γ = p[3]
        δ = p[4]
        du[1] = (α - β * y) * x
        du[2] = (δ * x - γ) * y
    end
end
p = [1.5, 1.0, 3.0, 1.0]
u0 = [1.0, 1.0]
prob = ODEProblem(lotka_volterra, u0, (0.0, 10.0), p)
prob_func = (prob, i, repeat) -&gt; remake(prob, p = rand(Float64, 4) .* p)
monteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)
@time sol = solve(monteprob, Tsit5(), EnsembleGPUArray(), trajectories = 10_000,
                  saveat = 1.0f0)</code></pre><p>Let&#39;s now use this in the ensembling method. We need to specify a <code>batch</code> for the number of ODEs solved at the same time, and pass in our ensembling method. The following is a GPU-accelerated uncertainty quantified estimate of the expectation of the solution:</p><pre><code class="language-julia hljs"># batchmode = EnsembleGPUArray() #where to pass this?
sol = solve(exprob, Koopman(), batch = 100, quadalg = CubaSUAVE())</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimization_under_uncertainty/">« Optimization Under Uncertainty</a><a class="docs-footer-nextpage" href="../process_noise/">Expectation of Process Noise »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Sunday 1 October 2023 15:19">Sunday 1 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
