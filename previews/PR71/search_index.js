var documenterSearchIndex = {"docs":
[{"location":"tutorials/introduction/#An-Introduction-to-Expectations-via-SciMLExpectations.jl","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"","category":"section"},{"location":"tutorials/introduction/#System-Model","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"System Model","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"First, lets consider the following linear model.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u = p u","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"f(u,p,t) = p.*u","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"We then wish to solve this model on the timespan t=0.0 to t=10.0, with an intial condition u0=10.0 and parameter p=-0.3. We can then setup the differential equations, solve, and plot as follows","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using DifferentialEquations, Plots\nu0 = [10.0]\np = [-0.3]\ntspan = (0.0,10.0)\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob)\nplot(sol)\nylims!(0.0,10.0)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"However, what if we wish to consider a random initial condition? Assume u0 is distributed uniformly from -10.0 to 10.0, i.e.,","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using Distributions\nu0_dist = [Uniform(-10.0,10.0)]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"We can then run a Monte Carlo simulation of 100,000 trajectories by solving an EnsembleProblem.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"prob_func(prob,i,repeat) = remake(prob, u0 = rand.(u0_dist))\nensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\n\nensemblesol = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=100000)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Plotting the first 250 trajectories produces","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"plot(ensemblesol, vars = (0,1), lw=1,alpha=0.1, label=nothing, idxs = 1:250)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Given the ensemble solution, we can then compute the expectation of a function gleft(cdotright) of the system state u at any time in the timespan, e.g. the state itself at t=4.0 as","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"g(sol) = sol(4.0)\nmean([g(sol) for sol in ensemblesol])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Alternatively, SciMLExpectations.jl offers a convenient interface for this type of calculation, expectation().","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using SciMLExpectations\nexpectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100000)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"expectation() takes the function of interest g, an ODEProblem, the initial conditions and parameters, and an AbstractExpectationAlgorithm. Here we use MonteCarlo() to use the Monte Carlo algorithm. Note that the initial conditions and parameters can be arrays that freely mix numeric and continuous distribution types from Distributions.jl. Recall, that u0_dist = [Uniform(-10.0,10.0)], while p = [-0.3]. From this specification, the expectation is solved as","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"mathbbEleftgleft(Xright)vert Xsim Pfright","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"where Pf is the \"push-forward\" density of the initial joint pdf f on initial conditions and parameters.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Alternatively, we could solve the same problem using the Koopman() algorithm.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"expectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Being that this system is linear, we can analytically compute the solution as a deterministic ODE with its initial condition set to the expectation of the initial condition, i.e.,","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"e^ptmathbbEleftu_0right","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"exp(p[1]*4.0)*mean(u0_dist[1])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"We see that for this case the Koopman() algorithm produces a more accurate solution than MonteCarlo(). Not only is it more accurate, it is also much faster","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"@time expectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100000)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"@time expectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Changing the distribution, we arrive at","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [Uniform(0.0,10.0)]\n@time expectation(g, prob, u0_dist, p, MonteCarlo(), Tsit5(); trajectories=100_000)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"and","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"@time expectation(g, prob, u0_dist, p, Koopman(), Tsit5())[1]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"where the analytical solution is","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"exp(p[1]*4.0)*mean(u0_dist[1])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Note that the Koopman() algorithm doesn't currently support infinite or semi-infinite integration domains, where the integration domain is determined by the extrema of the given distributions. So, trying to using a Normal distribution will produce NaN","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [Normal(3.0,2.0)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Here, the analytical solution is","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"exp(p[1]*4.0)*mean(u0_dist[1])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Using a truncated distribution will alleviate this problem. However, there is another gotcha. If a large majority of the probability mass of the distribution exists in a small region in the support, then the adaptive methods used to solve the expectation can \"miss\" the non-zero portions of the distribution and errantly return 0.0.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [truncated(Normal(3.0,2.0),-1000,1000)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"whereas truncating at pm 4sigma produces the correct result","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [truncated(Normal(3.0,2.0),-5,11)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"If a large truncation is required, it is best practice to center the distribution on the truncated interval. This is because many of the underlying quadrature algorithms use the center of the interval as an evaluation point.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [truncated(Normal(3.0,2.0),3-1000,3+1000)]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5())","category":"page"},{"location":"tutorials/introduction/#Vector-Valued-Functions","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Vector-Valued Functions","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"expectation() can also handle vector-valued functions. Simply pass the vector-valued function and set the nout kwarg to the length of the vector the function returns.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Here, we demonstrate this by computing the expectation of u at t=4.0s and t=6.0s","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"g(sol) = [sol(4.0)[1], sol(6.0)[1]]\nexpectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 2)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"with analytical solution","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"exp.(p.*[4.0,6.0])*mean(u0_dist[1])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"this can be used to compute the expectation at a range of times simultaneously","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"saveat = tspan[1]:.5:tspan[2]\ng(sol) = Matrix(sol)\nmean_koop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = length(saveat), saveat=saveat)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"We can then plot these values along with the analytical solution","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"plot(t->exp(p[1]*t)*mean(u0_dist[1]),tspan..., xlabel=\"t\", label=\"analytical\")\nscatter!(collect(saveat),mean_koop.u[:],marker=:o, label=nothing)","category":"page"},{"location":"tutorials/introduction/#Benefits-of-Using-Vector-Valued-Functions","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Benefits of Using Vector-Valued Functions","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"In the above examples we used vector-valued expectation calculations to compute the various expectations required. Alternatively, one could simply compute multiple scalar-valued expectations. However, in most cases it is more efficient to use the vector-valued form. This is especially true when the ODE to be solved is computationally expensive.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"To demonstrate this, lets compute the expectation of x, x^2, and x^3 using both approaches while counting the number of times g() is evaluated. This is the same as the number of simulation runs required to arrive at the solution. First, consider the scalar-valued approach. Here, we follow the same method as before, but we add a counter to our function evaluation that stores the number of function calls for each expectation calculation to an array.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"function g(sol, power, counter)\n    counter[power] = counter[power] + 1\n    sol(4.0)[1]^power\nend\n\ncounters = [0,0,0]\nx_koop = expectation(s->g(s,1,counters), prob, u0_dist, p, Koopman(), Tsit5())\nx2_koop = expectation(s->g(s,2,counters), prob, u0_dist, p, Koopman(), Tsit5())\nx3_koop = expectation(s->g(s,3,counters), prob, u0_dist, p, Koopman(), Tsit5())\ncounters","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Leading to a total of j sum(counters) function evaluations.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Now, lets compare this to the vector-valued approach","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"function g(sol, counter)\n    counter[1] = counter[1] + 1\n    v = sol(4.0)[1]\n    [v, v^2, v^3]\nend\n\ncounter = [0]\nexpectation(s->g(s,counter), prob, u0_dist, p, Koopman(), Tsit5(); nout = 3)\ncounter","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"This is j round(counter[1]/sum(counters)*100,digits=2)% the number of simulations required when using scalar-valued expectations. Note how the number of evaluations used in the vector-valued form is equivelent to the maximum number of evaluations for the 3 scalar-valued expectation calls.","category":"page"},{"location":"tutorials/introduction/#Higher-Order-Moments","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Higher-Order Moments","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Leveraging this vector-valued capability, we can also efficiently compute higher-order central moments.","category":"page"},{"location":"tutorials/introduction/#Variance","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Variance","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"The variance, or 2nd central moment, of a random variable X is defined as","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"mathrmVarleft(Xright)=mathbbEleftleft(X-muright)^2right","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"where","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"mu = mathbbEleftXright","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"The expression for the variance can be expanded to","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"mathrmVarleft(Xright)=mathbbEleftX^2right-mathbbEleftXright^2","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Using this, we define a function that returns the expectations of X and X^2 as a vector-valued function and then compute the variance from these","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"function g(sol)\n    x = sol(4.0)[1]\n    [x, x^2]\nend\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 2)\nmean_koop = koop[1]\nvar_koop = koop[2] - mean_koop^2","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"For a linear system, we can propagate the variance analytically as","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"e^2ptmathrmVarleft(u_0right)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"exp(2*p[1]*4.0)*var(u0_dist[1])","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"This can be computed at multiple time instances as well","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"saveat = tspan[1]:.5:tspan[2]\ng(sol) = [Matrix(sol)'; (Matrix(sol).^2)']\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = length(saveat)*2, saveat=saveat)\nμ = koop.u[1:length(saveat)]\nσ = sqrt.(koop.u[length(saveat)+1:end] - μ.^2)\n\nplot(t->exp(p[1]*t)*mean(u0_dist[1]),tspan..., ribbon = t->-sqrt(exp(2*p[1]*t)*var(u0_dist[1])), label=\"Analytical Mean, 1 std bounds\")\nscatter!(collect(saveat),μ,marker=:x, yerror = σ, c=:black, label = \"Koopman Mean, 1 std bounds\")","category":"page"},{"location":"tutorials/introduction/#Skewness","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Skewness","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"A similar approach can be used to compute skewness","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"function g(sol)\n    v = sol(4.0)[1]\n    [v, v^2, v^3]\nend\n\nkoop = expectation(g, prob, u0_dist, p, Koopman(), Tsit5(); nout = 3)\nmean_koop = koop[1]\nvar_koop = koop[2] - mean_koop^2\n(koop[3] - 3.0*mean_koop*var_koop - mean_koop^3) / var_koop^(3/2)","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"As the system is linear, we expect the skewness to be unchanged from the inital distribution. Becasue the distribution is a truncated Normal distribution centered on the mean, the true skewness is 0.0.","category":"page"},{"location":"tutorials/introduction/#nth-Central-Moment","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"nth Central Moment","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"SciMLExpectations provides a convenience function centralmoment around this approach for higher-order central moments. It takes an integer for the number of central moments you wish to compute. While the rest of the arguments are the same as for  expectation(). The following will return central moments 1-5.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"g(sol) = sol(4.0)[1]\ncentralmoment(5, g, prob, u0_dist, p, Koopman(), Tsit5(),\n                ireltol = 1e-9, iabstol = 1e-9)","category":"page"},{"location":"tutorials/introduction/#Batch-Mode","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"Batch-Mode","text":"","category":"section"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"It is also possible to solve the various simulations in parallel by using the batch kwarg and a batch-mode supported quadrature algorithm via the quadalg kwarg. To view the list of batch compatible quadrature algorithms, refer to Quadrature.jl. Note: Batch-mode operation is built on top of DifferentialEquation.jl's EnsembleProblem. See the EnsembleProblem documentation for additional options.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"The default quadtrature algorithm used by expectation() does not support batch-mode evaluation. So, we first load dependencies for additional quadrature algorithms","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using Quadrature, Cuba","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"We then solve our expectation as before using a batch=10 multi-thread parallelization via EnsembleThreads() of Cuba's SUAVE algorithm. However, in this case we introduce additional uncertainty in the model parameter.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"u0_dist = [truncated(Normal(3.0,2.0),-5,11)]\np_dist = [truncated(Normal(-.7, .1), -1,0)]\n\ng(sol) = sol(6.0)[1]\n\nexpectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleThreads();\n                quadalg = CubaSUAVE(), batch=10)[1]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Now, lets compare the performance of the batch and non-batch modes","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using BenchmarkTools\n\n@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5();\n                quadalg = CubaSUAVE())[1]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleThreads();\n                quadalg = CubaSUAVE(), batch=10)[1]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"It is also possible to parallelize across the GPU. However, one must be careful of the limitations of ensemble solutions with the GPU. Please refer to DiffEqGPU.jl for details.","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"Here we load DiffEqGPU and modify our problem to use Float32 and to put the ODE in the required GPU form","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"using DiffEqGPU\n\nfunction f(du, u,p,t)\n    @inbounds begin\n        du[1] = p[1]*u[1];\n    end\n    nothing\nend\n\nu0 = Float32[10.0]\np = Float32[-0.3]\ntspan = (0.0f0,10.0f0)\nprob = ODEProblem(f,u0,tspan,p)\n\ng(sol) = sol(6.0)[1]\n\nu0_dist = [truncated(Normal(3.0f0,2.0f0),-5f0,11f0)]\np_dist = [truncated(Normal(-.7f0, .1f0), -1f0,0f0)]\n\n@btime expectation(g, prob, u0_dist, p_dist, Koopman(), Tsit5(), EnsembleGPUArray();\n                   quadalg = CubaSUAVE(), batch=1000)[1]","category":"page"},{"location":"tutorials/introduction/","page":"An Introduction to Expectations via SciMLExpectations.jl","title":"An Introduction to Expectations via SciMLExpectations.jl","text":"The performance gains realized by leveraging batch GPU processing is problem dependent. In this case, the number of batch evaluations required to overcome the overhead of using the GPU exceeds the number of simulations required to converge to the quadrature solution.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/#Optimization-Under-Uncertainty-with-SciMLExpectations.jl","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"","category":"section"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"This tutorial gives and overview of how to leverage the efficient Koopman expectation method from SciMLExpectations to perform optimization under uncertainty. We demonstrate this by using a bouncing ball model with an uncertain model parameter. We also demonstrate its application to problems with probabilistic constraints, in particular a special class of constraints called chance constraints.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/#System-Model","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"System Model","text":"","category":"section"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"First lets consider a 2D bouncing ball, where the states are the horizontal position x, horizontal velocity dotx, vertical position y, and vertical velocity doty. This model has two system parameters, acceleration due to gravity and coefficient of restitution (models energy loss when the ball impacts the ground). We can simulate such a system using ContinuousCallback as","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"using OrdinaryDiffEq, Plots\n\nfunction ball!(du,u,p,t)\n    du[1] = u[2]\n    du[2] = 0.0\n    du[3] = u[4]\n    du[4] = -p[1]\nend\n\nground_condition(u,t,integrator) = u[3]\nground_affect!(integrator) = integrator.u[4] = -integrator.p[2] * integrator.u[4]\nground_cb = ContinuousCallback(ground_condition, ground_affect!)\n\nu0 = [0.0,2.0,50.0,0.0]\ntspan = (0.0,50.0)\np = [9.807, 0.9]\n\nprob = ODEProblem(ball!,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=ground_cb)\nplot(sol, vars=(1,3), label = nothing, xlabel=\"x\", ylabel=\"y\")","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"For this particular problem, we wish to measure the impact distance from a point y=25 on a wall at x=25. So, we introduce an additional callback that terminates the simulation on wall impact.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"stop_condition(u,t,integrator) = u[1] - 25.0\nstop_cb = ContinuousCallback(stop_condition, terminate!)\ncbs = CallbackSet(ground_cb, stop_cb)\n\ntspan = (0.0, 1500.0)\nprob = ODEProblem(ball!,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cbs)","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"To help visualize this problem, we plot as follows, where the star indicates a desired impace location","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"rectangle(xc, yc, w, h) = Shape(xc .+ [-w,w,w,-w]./2.0, yc .+ [-h,-h,h,h]./2.0)\n\nbegin\n    plot(sol, vars=(1,3), label=nothing, lw = 3, c=:black)\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing,c=:green)\n    ylims!(0.0,50.0)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/#Considering-Uncertainty","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Considering Uncertainty","text":"","category":"section"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We now wish to introduce uncertainty in p[2], the coefficient of restitution. This is defined via a continuous univiate distribution from Distributions.jl. We can then run a Monte Carlo simulation of 100,000 trajectories via the EnsembleProblem interface.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"using Distributions\n\ncor_dist = truncated(Normal(0.9, 0.02), 0.9-3*0.02, 1.0)\ntrajectories = 100000\n\nprob_func(prob,i,repeat) = remake(prob, p = [p[1], rand(cor_dist)])\nensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nensemblesol = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=trajectories, callback=cbs)\n\nbegin # plot\n    plot(ensemblesol, vars = (1,3), lw=1,alpha=0.2, label=nothing, idxs = 1:350)\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing, c=:green)\n    plot!(sol, vars=(1,3), label=nothing, lw = 3, c=:black, ls=:dash)\n    xlims!(0.0,27.5)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Here, we plot the first 350 Monte Carlo simulations along with the trajectory corrresponding to the mean of the distribution (dashed line).","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We now wish to compute the expected squared impact distance from the star. This is called an \"observation\" of our system or an \"observable\" of interest.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We define this observable as","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"obs(sol) = abs2(sol[3,end]-25)","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"With the observable defined, we can compute the expected squared miss distance from our Monte Carlo simulation results as","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"mean_ensemble = mean([obs(sol) for sol in ensemblesol])","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Alternatively, we can use the Koopman() algorithm in SciMLExpectations.jl to compute this expectation much more efficiently as","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"using SciMLExpectations\n\np_uncertain = [9.807, cor_dist]\nexpectation(obs, prob, u0, p_uncertain, Koopman(), Tsit5();\n            ireltol = 1e-5, callback=cbs)","category":"page"},{"location":"tutorials/optimization_under_uncertainty/#Optimization-Under-Uncertainty","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty","text":"","category":"section"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We now wish to optimize the initial position (x_0y_0) and horizontal velocity (dotx_0) of the system to minimize the expected squared miss distance from the star, where x_0inleft-1000right, y_0inleft13right, and dotx_0inleft1050right. We will demonstrate this using a gradient-based optimization approach from NLopt.jl using ForwardDiff.jl AD through the expectation calculation.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"First, we load the required packages and define our loss function","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"using NLopt, DiffEqSensitivity, ForwardDiff\n\nmake_u0(θ) = [θ[1],θ[2],θ[3], 0.0]\n\nfunction 𝔼_loss(θ)   # \\bbE\n    u0 = make_u0(θ)\n    expectation(obs, prob, u0, p_uncertain, Koopman(), Tsit5();\n                 ireltol = 1e-5, callback=cbs)[1]\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"NLopt requires that this loss function return the loss as above, but also do an inplace update of the gradient. So, we wrap this function to put it in the form required by NLopt.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"function 𝔼_loss_nlopt(x,∇)\n    length(∇) > 0 ? ForwardDiff.gradient!(∇, 𝔼_loss,x) : nothing\n    𝔼_loss(x)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We then optimize using the Method of Moving Asymptotes algorithm (:LD_MMA)","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"opt = Opt(:LD_MMA, 3)\nopt.lower_bounds = [-100.0,1.0, 10.0]\nopt.upper_bounds = [0.0,3.0, 50.0]\nopt.xtol_rel = 1e-3\nopt.min_objective = 𝔼_loss_nlopt\n(minf,minx,ret) = NLopt.optimize(opt, [-1.0, 2.0, 50.0])","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Let's now visualize 350 Monte Carlo simulations","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"ensembleprob = EnsembleProblem(remake(prob,u0 = make_u0(minx)),prob_func=prob_func)\nensemblesol = solve(ensembleprob,Tsit5(),EnsembleThreads(), trajectories=100_000, callback=cbs)\n\nbegin\n    plot(ensemblesol, vars = (1,3), lw=1,alpha=0.1, label=nothing, idxs = 1:350)\n    plot!(solve(remake(prob, u0=make_u0(minx)),Tsit5(), callback=cbs),\n            vars=(1,3),label = nothing, c=:black, lw=3,ls=:dash)\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing,c=:green)\n    ylims!(0.0,50.0)\n    xlims!(minx[1], 27.5)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Looks pretty good! But, how long did it take? Let's benchmark.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"using BenchmarkTools\n\n@btime NLopt.optimize($opt, $[-1.0, 2.0, 50.0])","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Not bad for bound constrained optimization under uncertainty of a hybrid system!","category":"page"},{"location":"tutorials/optimization_under_uncertainty/#Probabilistic-Constraints","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Probabilistic Constraints","text":"","category":"section"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"With this approach we can also consider probabilistic constraints. Let us now consider a wall at x=20 with height 25.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"constraint = [20.0, 25.0]\nbegin\n    plot(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!([constraint[1], constraint[1]],[0.0,constraint[2]], lw=5, c=:black, label=nothing)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing,c=:green)\n    ylims!(0.0,50.0)\n    xlims!(minx[1], 27.5)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We now wish to minimize the same loss function as before, but introduce an inequality constraint such that the solution must have less than a 1% chance of colliding with the wall at x=20. This class of probabilistic constraints is called a chance constraint.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"To do this, we first introduce a new callback and solve the system using the previous optimal solution","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"constraint_condition(u,t,integrator) = u[1] - constraint[1]\nconstraint_affect!(integrator) = integrator.u[3] < constraint[2] ? terminate!(integrator) : nothing\nconstraint_cb = ContinuousCallback(constraint_condition, constraint_affect!, save_positions=(true,false));\nconstraint_cbs = CallbackSet(ground_cb, stop_cb, constraint_cb)\n\nensemblesol = solve(ensembleprob,Tsit5(),EnsembleThreads(), trajectories=350, callback=constraint_cbs, maxstep=0.1)\n\nbegin\n    plot(ensemblesol, vars = (1,3), lw=1,alpha=0.1, label=nothing)\n    plot!(solve(remake(prob, u0=make_u0(minx)),Tsit5(), callback=constraint_cbs),\n            vars=(1,3),label = nothing, c=:black, lw=3, ls=:dash)\n\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    plot!([constraint[1], constraint[1]],[0.0,constraint[2]], lw=5, c=:black)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing,c=:green)\n    ylims!(0.0,50.0)\n    xlims!(minx[1], 27.5)\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"That doesn't look good!","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We now need a second observable for the system. In order to compute a probability of impact, we use an indicator function for if a trajectory impacts the wall. In other words, this functions returns 1 if the trajectory hits the wall and 0 otherwise.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"constraint_obs(sol) = sol[1,end] ≈ constraint[1] ? one(sol[1,end]) : zero(sol[1,end])","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Using the previously computed optimal initial conditions, lets compute the probability of hitting this wall","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"expectation(constraint_obs, prob, make_u0(minx), p_uncertain, Koopman(), Tsit5();\n            ireltol= 1e-9, iabstol = 1e-9, callback=constraint_cbs)[1]","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We then setup the constraint function for NLopt just as before.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"function 𝔼_constraint(θ)\n    u0 = [θ[1],θ[2],θ[3], 0.0]\n    expectation(constraint_obs, prob, u0, p_uncertain, Koopman(), Tsit5(),\n                ireltol= 1e-9, iabstol = 1e-9,callback=constraint_cbs)[1]\nend\n\nfunction 𝔼_constraint_nlopt(x,∇)\n    length(∇) > 0 ? ForwardDiff.gradient!(∇, 𝔼_constraint,x) : nothing\n    𝔼_constraint(x) - 0.01\nend","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Note that NLopt requires the constraint function to be of the form g(x) leq 0. Hence, why we return 𝔼_constraint(x) - 0.01 for the 1% chance constraint.","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"The rest of the NLopt setup looks the same as before with the exception of adding the inequality constraint","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"opt = Opt(:LD_MMA, 3)\nopt.lower_bounds = [-100.0, 1.0, 10.0]\nopt.upper_bounds = [0.0, 3.0, 50.0]\nopt.xtol_rel = 1e-3\nopt.min_objective = 𝔼_loss_nlopt\ninequality_constraint!(opt,𝔼_constraint_nlopt, 1e-5)\n(minf2,minx2,ret2) = NLopt.optimize(opt, [-1.0, 2.0, 50.0])","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"The probability of impacting the wall is now","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"λ = 𝔼_constraint(minx2)","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"We can check if this is within tolerance by","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"λ - 0.01 <= 1e-5","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"Again, we plot some Monte Carlo simulations from this result as follows","category":"page"},{"location":"tutorials/optimization_under_uncertainty/","page":"Optimization Under Uncertainty with SciMLExpectations.jl","title":"Optimization Under Uncertainty with SciMLExpectations.jl","text":"ensembleprob = EnsembleProblem(remake(prob,u0 = make_u0(minx2)),prob_func=prob_func)\nensemblesol = solve(ensembleprob,Tsit5(),EnsembleThreads(),\n                    trajectories=350, callback=constraint_cbs)\n\nbegin\n    plot(ensemblesol, vars = (1,3), lw=1,alpha=0.1, label=nothing)\n    plot!(solve(remake(prob, u0=make_u0(minx2)),Tsit5(), callback=constraint_cbs),\n            vars=(1,3),label = nothing, c=:black, lw=3, ls=:dash)\n    plot!([constraint[1], constraint[1]],[0.0,constraint[2]], lw=5, c=:black)\n\n    xlabel!(\"x [m]\")\n    ylabel!(\"y [m]\")\n    plot!(rectangle(27.5, 25, 5, 50), c=:red, label = nothing)\n    scatter!([25],[25],marker=:star, ms=10, label = nothing,c=:green)\n    ylims!(0.0,50.0)\n    xlims!(minx[1], 27.5)\nend","category":"page"},{"location":"tutorials/gpu_bayesian/#GPU-Accelerated-Data-Driven-Bayesian-Uncertainty-Quantification-with-Koopman-Operators","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"","category":"section"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"What if you have data and a general model and would like to evaluate the probability that the fitted model outcomes would have had a given behavior? The purpose of this tutorial is to demonstrate a fast workflow for doing exactly this. It composes together a few different pieces of the SciML ecosystem:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Parameter estimation with uncertainty with Bayesian differential equations by integrating the differentiable differential equation solvers with the Turing.jl library.\nFast calculation of probabilistic estimates of differential equation solutions with parametric uncertainty using the Koopman expectation.\nGPU-acceleration of batched differential equation solves.","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Let's dive right in.","category":"page"},{"location":"tutorials/gpu_bayesian/#Bayesian-Parameter-Estimation-with-Uncertainty","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"Bayesian Parameter Estimation with Uncertainty","text":"","category":"section"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Let's start by importing all of the necessary libraries:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"using Turing, Distributions, DifferentialEquations\nusing MCMCChains, Plots, StatsPlots\nusing Random\nusing SciMLExpectations\nusing KernelDensity, SciMLExpectations\nusing Cuba, DiffEqGPU\n\nRandom.seed!(1);","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"For this tutorial we will use the Lotka-Volterra equation:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"function lotka_volterra(du,u,p,t)\n  @inbounds begin\n      x = u[1]\n      y = u[2]\n      α = p[1]\n      β = p[2]\n      γ = p[3]\n      δ = p[4]\n      du[1] = (α - β*y)*x\n      du[2] = (δ*x - γ)*y\n  end\nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\nprob1 = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)\nsol = solve(prob1,Tsit5())\nplot(sol)","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"From the Lotka-Volterra equation we will generate a dataset with known parameters:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"sol1 = solve(prob1,Tsit5(),saveat=0.1)","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Now let's assume our dataset should have noise. We can add this noise in and plot the noisy data against the generating set:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"odedata = Array(sol1) + 0.8 * randn(size(Array(sol1)))\nplot(sol1, alpha = 0.3, legend = false); scatter!(sol1.t, odedata')","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Now let's assume that all we know is the data odedata and the model form. What we want to do is use the data to inform us of the parameters, but also get a probabilistic sense of the uncertainty around our parameter estimate. This is done via Bayesian estimation. For a full look at Bayesian estimation of differential equations, look at the Bayesian differential equation tutorial from Turing.jl.","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Following that tutorial, we choose a set of priors and perform NUTS sampling to arrive at the MCMC chain:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),1.0,2.0)\n    β ~ truncated(Normal(1.2,0.5),0.5,1.5)\n    γ ~ truncated(Normal(3.0,0.5),2,4)\n    δ ~ truncated(Normal(1.0,0.5),0.5,1.5)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading.\nchain = mapreduce(c -> sample(model, NUTS(.45),1000), chainscat, 1:3)","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"This chain gives a discrete approximation to the probability distribution of our desired quantites. We can plot the chains to see this distributions in action:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"plot(chain)","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Great! From our data we have arrived at a probability distribution for the our parameter values.","category":"page"},{"location":"tutorials/gpu_bayesian/#Evaluating-Model-Hypotheses-with-the-Koopman-Expectation","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"Evaluating Model Hypotheses with the Koopman Expectation","text":"","category":"section"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Now let's try and ask a question: what is the expected value of x (the first term in the differential equation) at time t=10 given the known uncertainties in our parameters? This is a good tutorial question because all other probabilistic statements can be phrased similarly. Asking a question like, \"what is the probability that x(T) > 1 at the final time T?\", can similarly be phrased as an expected value (probability statements are expected values of characteristic functions which are 1 if true 0 if false). So in general, the kinds of questions we want to ask and answer are expectations about the solutions of the differential equation.","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"The trivial to solve this problem is to sample 100,000 sets of parameters from our parameter distribution given by the Bayesian estimation, solve the ODE 100,000 times, and then take the average. But is 100,000 ODE solves enough? Well it's hard to tell, and even then, the convergence of this approach is slow. This is the Monte Carlo approach and it converges to the correct answer by sqrt(N). Slow.","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"However, the Koopman expectation can converge with much fewer points, allowing the use of higher order quadrature methods to converge exponentially faster in many cases. To use the Koopman expectation functionality provided by SciMLExpectations.jl, we first need to define our observable function g. This function designates the thing about the solution we wish to calculate the expectation of. Thus for our question \"what is the expected value of xat time t=10?\", we would simply use:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"function g(sol)\n    sol[1,end]\nend","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Now we need to use the expectation call, where we need to provide our initial condition and parameters as probability distirbutions. For this case, we will use the same constant u0 as before. But, let's turn our Bayesian MCMC chains into distributions through kernel density estimation (the plots of the distribution above are just KDE plots!).","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"p_kde = [kde(vec(Array(chain[:α]))),kde(vec(Array(chain[:β]))),\n         kde(vec(Array(chain[:γ]))),kde(vec(Array(chain[:δ])))]","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Now that we have our observable and our uncertainty distributions, let's calculate the expected value:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"expect = expectation(g, prob1, u0, p_kde, Koopman(), Tsit5(), quadalg = CubaCuhre())","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Note how that gives the expectation and a residual for the error bound!","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"expect.resid","category":"page"},{"location":"tutorials/gpu_bayesian/#GPU-Accelerated-Expectations","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Expectations","text":"","category":"section"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Are we done? No, we need to add some GPUs! As mentioned earlier, probability calculations can take quite a bit of ODE solves, so let's parallelize across the parameters. DiffEqGPU.jl allows you to GPU-parallelize across parameters by using the Ensemble interface. Note that you do not have to do any of the heavy lifting: all of the conversion to GPU kernels is done automaticaly by simply specifying EnsembleGPUArray as the ensembling method. For example:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"function lotka_volterra(du,u,p,t)\n  @inbounds begin\n      x = u[1]\n      y = u[2]\n      α = p[1]\n      β = p[2]\n      γ = p[3]\n      δ = p[4]\n      du[1] = (α - β*y)*x\n      du[2] = (δ*x - γ)*y\n  end\nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\nprob = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)\nprob_func = (prob,i,repeat) -> remake(prob,p=rand(Float64,4).*p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy=false)\n@time sol = solve(monteprob,Tsit5(),EnsembleGPUArray(),trajectories=10_000,saveat=1.0f0)","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"Let's now use this in the ensembling method. We need to specify a batch for the number of ODEs solved at the same time, and pass in our enembling method. The following is a GPU-accelerated uncertainty quanitified estimate of the expectation of the solution:","category":"page"},{"location":"tutorials/gpu_bayesian/","page":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","title":"GPU-Accelerated Data-Driven Bayesian Uncertainty Quantification with Koopman Operators","text":"expectation(g, prob1, u0, p_kde, Koopman(), Tsit5(), EnsembleGPUArray(), batch=100, quadalg = CubaCuhre())","category":"page"},{"location":"manual/problem/#ExpectationProblem","page":"ExpectationProblem","title":"ExpectationProblem","text":"","category":"section"},{"location":"manual/problem/","page":"ExpectationProblem","title":"ExpectationProblem","text":"ExpectationProblem\nSystemMap","category":"page"},{"location":"manual/problem/#SciMLExpectations.ExpectationProblem","page":"ExpectationProblem","title":"SciMLExpectations.ExpectationProblem","text":"ExpectationProblem(S, g, h, pdist, params, nout)\nExpectationProblem(g, pdist, params; nout = 1)\nExpectationProblem(sm::SystemMap, g, h, d; nout = 1)\n\nDefines ∫ g(S(h(x,u0,p)))*f(x)dx\n\nArguments\n\nLet 𝕏 = uncertainty space, 𝕌 = Initial condition space, ℙ = model parameter space\n\nS: 𝕌 × ℙ → 𝕌.\ng: 𝕌 × ℙ → ℝⁿᵒᵘᵗ also known as the observables or output function.\nh: 𝕏 × 𝕌 × ℙ → 𝕌 × ℙ, cov(input_func).\npdf(d,x): 𝕏 → ℝ the uncertainty distribution of the initial states.\nparams\nnout\n\n\n\n\n\n","category":"type"},{"location":"manual/problem/#SciMLExpectations.SystemMap","page":"ExpectationProblem","title":"SciMLExpectations.SystemMap","text":"SystemMap(prob, args...; kwargs...)\n\nRepresentation of a system solution map for a given prob::DEProblem. args and kwargs are forwarded to the equation solver.\n\n\n\n\n\n","category":"type"},{"location":"#SciMLExpectations.jl:-Expectated-Values-of-Simulations-and-Uncertainty-Quantification","page":"Home","title":"SciMLExpectations.jl: Expectated Values of Simulations and Uncertainty Quantification","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SciMLExpectations.jl is a package for quantifying the uncertainties of simulations by calculating the expectations of observables with respect to input uncertainties. Its goal is to make it fast and easy to compute solution moments in a differentiable way in order to enable fast optimization under uncertainty.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install SciMLExpectations.jl, use the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"SciMLExpectations\")","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"manual/algorithms/#Expectation-Algorithms","page":"Expectation Algorithms","title":"Expectation Algorithms","text":"","category":"section"},{"location":"manual/algorithms/","page":"Expectation Algorithms","title":"Expectation Algorithms","text":"MonteCarlo\nKoopman","category":"page"},{"location":"manual/algorithms/#SciMLExpectations.MonteCarlo","page":"Expectation Algorithms","title":"SciMLExpectations.MonteCarlo","text":"MonteCarlo(trajectories::Int)\n\n\n\n\n\n","category":"type"},{"location":"manual/algorithms/#SciMLExpectations.Koopman","page":"Expectation Algorithms","title":"SciMLExpectations.Koopman","text":"Koopman()\n\n\n\n\n\n","category":"type"},{"location":"manual/solve/#Solving-Expectation-Problems","page":"Solving Expectation Problems","title":"Solving Expectation Problems","text":"","category":"section"},{"location":"manual/solve/","page":"Solving Expectation Problems","title":"Solving Expectation Problems","text":"solve(prob::ExpectationProblem, expalg, args...)","category":"page"}]
}
